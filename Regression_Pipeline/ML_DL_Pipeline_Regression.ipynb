{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self, filepath, columns_to_drop, target_column, test_size):\n",
    "        self.data = None\n",
    "        self.filepath = filepath\n",
    "        self.col_to_drop = columns_to_drop\n",
    "        self.trgt_col = str(target_column)\n",
    "        self.test_size = test_size\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        \n",
    "    def __read_file(self):\n",
    "        df = pd.read_csv(self.filepath)\n",
    "        return df\n",
    "    \n",
    "    def __fill_missing_values(self):\n",
    "        null_cols = self.data.columns[self.data.isna().any()].tolist()\n",
    "        for item in null_cols:\n",
    "            if ((self.data[item].nunique() < 10) or (self.data[item].dtypes == object)):\n",
    "                # taking only binary value columns and replacing the missing values with mode\n",
    "                self.data[item].fillna(self.data[item].mode()[0],inplace=True)\n",
    "            elif self.data[item].dtype in (['int64', 'float64']):\n",
    "                self.data[item].fillna(self.data[item].mean(),inplace=True)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    def __dependent_independent_split(self):\n",
    "        self.data = self.data.drop(self.col_to_drop, axis=1)\n",
    "        X = self.data.drop(self.trgt_col, axis=1)\n",
    "        y = self.data[self.trgt_col]\n",
    "        return X, y\n",
    "    \n",
    "    def __label_encode(self, y):\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "        return y\n",
    "        \n",
    "    def __one_hot_encode(self, x):\n",
    "        x = pd.get_dummies(x, drop_first=True)\n",
    "        return x\n",
    "        \n",
    "    def __train_test_split(self, X, y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=0)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    def standard_scaling(self):\n",
    "        scale = StandardScaler()\n",
    "        self.X_train = scale.fit_transform(self.X_train)\n",
    "        self.X_test = scale.transform(self.X_test)\n",
    "        \n",
    "    def min_max_scaling(self):\n",
    "        scale = MinMaxScaler()\n",
    "        self.X_train = scale.fit_transform(self.X_train)\n",
    "        self.X_test = scale.transform(self.X_test)\n",
    "        \n",
    "        \n",
    "    def robust_scaling(self):\n",
    "        scale = RobustScaler(quantile_range=(25.0, 75.0))\n",
    "        self.X_train = scale.fit_transform(self.X_train)\n",
    "        self.X_test = scale.transform(self.X_test)\n",
    "    \n",
    "    def data_processing_call(self):\n",
    "        self.data = self.__read_file()\n",
    "        self.__fill_missing_values()\n",
    "        X, y = self.__dependent_independent_split()\n",
    "        X = self.__one_hot_encode(X)\n",
    "        X.sort_index(axis=1, inplace=True)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.__train_test_split(X, y)\n",
    "        self.min_max_scaling()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models(Preprocessing):\n",
    "    def __init__(self, filepath, columns_to_drop, target_column, test_size, model_list, hyperparameters=False):\n",
    "        super().__init__(filepath, columns_to_drop, target_column, test_size)\n",
    "        self.params = {}\n",
    "        self.classifiers = model_list\n",
    "        self.hyperparameters = hyperparameters\n",
    "        \n",
    "    def data_processing(self):\n",
    "        self.data_processing_call()\n",
    "        \n",
    "    def lr_model(self):\n",
    "        self.params[\"LR\"] = {}\n",
    "        model = LinearRegression()\n",
    "        if not self.hyperparameters:\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            return model\n",
    "        else:\n",
    "            model_search = self.hyperparameter_tuning(model)\n",
    "            return model_search\n",
    "        \n",
    "    def knn_model(self):\n",
    "        self.params[\"KNN\"] = {}\n",
    "        model = KNeighborsRegressor()\n",
    "        if not self.hyperparameters:\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            return model\n",
    "        else:\n",
    "            model_search = self.hyperparameter_tuning(model)\n",
    "            return model_search\n",
    "    \n",
    "    def rf_model(self):\n",
    "        self.params[\"RF\"] = {}\n",
    "        model = RandomForestRegressor()\n",
    "        if not self.hyperparameters:\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            return model\n",
    "        else:\n",
    "            model_search = self.hyperparameter_tuning(model)\n",
    "            return model_search\n",
    "        \n",
    "    def ann_model(self):\n",
    "        self.params[\"ANN\"] = {}\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_shape=self.input_shape, activation='relu'))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        if not self.hyperparameters:\n",
    "            model.fit(self.X_train, self.y_train, epochs=self.epochs, \n",
    "                      batch_size=self.batch_size, callbacks=self.callback, \n",
    "                      validation_data=(self.X_test, self.y_test))\n",
    "            return model\n",
    "        else:\n",
    "            model_search = self.hyperparameter_tuning(model)\n",
    "            return model_search\n",
    "    \n",
    "      \n",
    "    def set_dl_parameters(self):\n",
    "        self.input_shape = (12,)\n",
    "        self.loss = 'mean_squared_error'\n",
    "        opt = SGD(lr=0.01, decay=0.01 / 40, momentum=0.9, nesterov=True)\n",
    "        self.optimizer = \"adam\"\n",
    "        self.epochs = 40\n",
    "        self.batch_size = 10\n",
    "        self.checkpoint_filepath = \"my_best_model.hdf5\"\n",
    "        model_checkpoint_callback = ModelCheckpoint(filepath=self.checkpoint_filepath,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    monitor='val_loss',\n",
    "                                                    mode='min',\n",
    "                                                    verbose=1,\n",
    "                                                    save_best_only=True)\n",
    "        \n",
    "        self.callback = [model_checkpoint_callback]\n",
    "    \n",
    "    def dl_modelling(self):\n",
    "        ml_model_scores_df = pd.DataFrame(columns=[\"Model_Name\", \"MAE\", \"RMSE\", \"R2\"])\n",
    "        scores = {}\n",
    "        model_obj = {}\n",
    "        self.set_dl_parameters()\n",
    "        for idx, model_name in enumerate(self.classifiers):\n",
    "            if model_name == \"ANN\":\n",
    "                model_trained = self.ann_model()\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            model_obj[model_name] = model_trained\n",
    "            \n",
    "            model_trained.load_weights(self.checkpoint_filepath)\n",
    "            # load_model(checkpoint_filepath)\n",
    "            \n",
    "            y_pred = model_trained.predict(self.X_test)\n",
    "            \n",
    "            mae, rmse, r2 = self.performance_metrics(self.y_test, y_pred)\n",
    "            ml_model_scores_df.loc[idx] = [model_name, mae, rmse, r2]\n",
    "            \n",
    "        return ml_model_scores_df, model_obj\n",
    "    \n",
    "    def ml_modelling(self):\n",
    "        ml_model_scores_df = pd.DataFrame(columns=[\"Model_Name\", \"MAE\", \"RMSE\", \"R2\"])\n",
    "        scores = {}\n",
    "        model_obj = {}\n",
    "        for idx, model_name in enumerate(self.classifiers):\n",
    "            if model_name == \"LR\":\n",
    "                model_trained = self.lr_model()\n",
    "            elif model_name == \"KNN\":\n",
    "                model_trained = self.knn_model()\n",
    "            elif model_name == \"RF\":\n",
    "                model_trained = self.rf_model()\n",
    "            else:\n",
    "                print(f\"Not a valid ml model: {model_name}\")\n",
    "                continue\n",
    "            \n",
    "            model_obj[model_name] = model_trained\n",
    "            \n",
    "            y_pred = model_trained.predict(self.X_test)\n",
    "            mae, rmse, r2 = self.performance_metrics(self.y_test, y_pred)\n",
    "            ml_model_scores_df.loc[idx] = [model_name, mae, rmse, r2]\n",
    "        \n",
    "        return ml_model_scores_df, model_obj\n",
    "\n",
    "    def hyperparameter_tuning(self, model):\n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "        search = RandomizedSearchCV(model, self.params, cv=cv, scoring='accuracy')\n",
    "        result = search.fit(self.X_train, self.y_train)\n",
    "        accuracy = result.best_score_\n",
    "        best_params = result.best_params_\n",
    "        return result\n",
    "    \n",
    "    def performance_metrics(self, y_true, y_pred):\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        return mae, rmse, r2\n",
    "    \n",
    "    def ml_model_save(self, model):\n",
    "        with (open(\"best_model.pickle\", \"wb\")) as file:\n",
    "            pickle.dump(model, file)\n",
    "            \n",
    "    def dl_model_save(self, model):\n",
    "        model.save(\"best_model.h5\")\n",
    "            \n",
    "    def save_model_scores(self, model_scores_df):\n",
    "        model_scores_df.to_excel(\"All_Model_Performances.xlsx\", index=False)\n",
    "    \n",
    "    def model_calls(self):\n",
    "        ml_model_scores_df, model_obj = self.ml_modelling()\n",
    "        dl_model_scores_df, dl_model_obj = self.dl_modelling()\n",
    "        \n",
    "        ml_model_scores_df = ml_model_scores_df.append(dl_model_scores_df, ignore_index=True)\n",
    "        model_obj.update(dl_model_obj)\n",
    "        \n",
    "        ml_model_scores_df = ml_model_scores_df.sort_values(by = [\"MAE\", \"RMSE\", \"R2\"], ignore_index=True)\n",
    "        best_model_name = ml_model_scores_df[\"Model_Name\"][0]\n",
    "        best_model = model_obj[best_model_name]\n",
    "        print(ml_model_scores_df)\n",
    "        if (best_model_name in list(dl_model_scores_df[\"Model_Name\"])):\n",
    "            self.dl_model_save(best_model)\n",
    "        else:\n",
    "            self.ml_model_save(best_model)\n",
    "            \n",
    "        self.save_model_scores(ml_model_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"dataset/boston.csv\"\n",
    "columns_to_drop = []\n",
    "target_column = \"MEDV\"\n",
    "test_size = 0.3\n",
    "model_list = [\"LR\", \"KNN\", \"RF\", \"ANN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a valid ml model: ANN\n",
      "Epoch 1/40\n",
      "36/36 [==============================] - 1s 10ms/step - loss: 599.5808 - accuracy: 0.0000e+00 - val_loss: 548.5964 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 548.59637, saving model to my_best_model.hdf5\n",
      "Epoch 2/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 599.3378 - accuracy: 0.0000e+00 - val_loss: 489.7567 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss improved from 548.59637 to 489.75674, saving model to my_best_model.hdf5\n",
      "Epoch 3/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 481.1365 - accuracy: 0.0000e+00 - val_loss: 324.6776 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss improved from 489.75674 to 324.67755, saving model to my_best_model.hdf5\n",
      "Epoch 4/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 285.7599 - accuracy: 0.0000e+00 - val_loss: 160.4109 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss improved from 324.67755 to 160.41087, saving model to my_best_model.hdf5\n",
      "Epoch 5/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 162.2726 - accuracy: 0.0000e+00 - val_loss: 137.5281 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss improved from 160.41087 to 137.52812, saving model to my_best_model.hdf5\n",
      "Epoch 6/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 146.1303 - accuracy: 0.0000e+00 - val_loss: 116.7515 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss improved from 137.52812 to 116.75148, saving model to my_best_model.hdf5\n",
      "Epoch 7/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 126.8838 - accuracy: 0.0000e+00 - val_loss: 99.7719 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss improved from 116.75148 to 99.77189, saving model to my_best_model.hdf5\n",
      "Epoch 8/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 104.5736 - accuracy: 0.0000e+00 - val_loss: 84.1467 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss improved from 99.77189 to 84.14669, saving model to my_best_model.hdf5\n",
      "Epoch 9/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 76.0631 - accuracy: 0.0000e+00 - val_loss: 73.1756 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss improved from 84.14669 to 73.17559, saving model to my_best_model.hdf5\n",
      "Epoch 10/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 67.3588 - accuracy: 0.0000e+00 - val_loss: 65.0990 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss improved from 73.17559 to 65.09895, saving model to my_best_model.hdf5\n",
      "Epoch 11/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 57.8157 - accuracy: 0.0000e+00 - val_loss: 59.3530 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_loss improved from 65.09895 to 59.35305, saving model to my_best_model.hdf5\n",
      "Epoch 12/40\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 60.2012 - accuracy: 0.0000e+00 - val_loss: 56.2849 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_loss improved from 59.35305 to 56.28492, saving model to my_best_model.hdf5\n",
      "Epoch 13/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 48.3024 - accuracy: 0.0000e+00 - val_loss: 53.9243 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_loss improved from 56.28492 to 53.92427, saving model to my_best_model.hdf5\n",
      "Epoch 14/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 40.7320 - accuracy: 0.0000e+00 - val_loss: 52.7792 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss improved from 53.92427 to 52.77916, saving model to my_best_model.hdf5\n",
      "Epoch 15/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 41.8818 - accuracy: 0.0000e+00 - val_loss: 49.6387 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_loss improved from 52.77916 to 49.63866, saving model to my_best_model.hdf5\n",
      "Epoch 16/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 40.2204 - accuracy: 0.0000e+00 - val_loss: 47.8805 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00016: val_loss improved from 49.63866 to 47.88050, saving model to my_best_model.hdf5\n",
      "Epoch 17/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 35.8457 - accuracy: 0.0000e+00 - val_loss: 45.9797 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00017: val_loss improved from 47.88050 to 45.97975, saving model to my_best_model.hdf5\n",
      "Epoch 18/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 39.9151 - accuracy: 0.0000e+00 - val_loss: 43.7897 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_loss improved from 45.97975 to 43.78966, saving model to my_best_model.hdf5\n",
      "Epoch 19/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 37.1925 - accuracy: 0.0000e+00 - val_loss: 42.0068 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_loss improved from 43.78966 to 42.00677, saving model to my_best_model.hdf5\n",
      "Epoch 20/40\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 37.1957 - accuracy: 0.0000e+00 - val_loss: 40.4077 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss improved from 42.00677 to 40.40770, saving model to my_best_model.hdf5\n",
      "Epoch 21/40\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 29.7978 - accuracy: 0.0000e+00 - val_loss: 39.6279 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_loss improved from 40.40770 to 39.62793, saving model to my_best_model.hdf5\n",
      "Epoch 22/40\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 26.5592 - accuracy: 0.0000e+00 - val_loss: 37.3045 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_loss improved from 39.62793 to 37.30454, saving model to my_best_model.hdf5\n",
      "Epoch 23/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 25.1107 - accuracy: 0.0000e+00 - val_loss: 36.4407 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_loss improved from 37.30454 to 36.44068, saving model to my_best_model.hdf5\n",
      "Epoch 24/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 24.2580 - accuracy: 0.0000e+00 - val_loss: 34.9876 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_loss improved from 36.44068 to 34.98756, saving model to my_best_model.hdf5\n",
      "Epoch 25/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 23.7000 - accuracy: 0.0000e+00 - val_loss: 33.7890 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_loss improved from 34.98756 to 33.78899, saving model to my_best_model.hdf5\n",
      "Epoch 26/40\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 19.5690 - accuracy: 0.0000e+00 - val_loss: 33.4235 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_loss improved from 33.78899 to 33.42347, saving model to my_best_model.hdf5\n",
      "Epoch 27/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 24.5184 - accuracy: 0.0000e+00 - val_loss: 32.6153 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00027: val_loss improved from 33.42347 to 32.61528, saving model to my_best_model.hdf5\n",
      "Epoch 28/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 25.1941 - accuracy: 0.0000e+00 - val_loss: 31.8245 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00028: val_loss improved from 32.61528 to 31.82445, saving model to my_best_model.hdf5\n",
      "Epoch 29/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 25.3096 - accuracy: 0.0000e+00 - val_loss: 31.6798 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_loss improved from 31.82445 to 31.67976, saving model to my_best_model.hdf5\n",
      "Epoch 30/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 26.6740 - accuracy: 0.0000e+00 - val_loss: 30.7686 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00030: val_loss improved from 31.67976 to 30.76856, saving model to my_best_model.hdf5\n",
      "Epoch 31/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 28.6840 - accuracy: 0.0000e+00 - val_loss: 30.5092 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00031: val_loss improved from 30.76856 to 30.50919, saving model to my_best_model.hdf5\n",
      "Epoch 32/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 24.2507 - accuracy: 0.0000e+00 - val_loss: 29.6909 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00032: val_loss improved from 30.50919 to 29.69091, saving model to my_best_model.hdf5\n",
      "Epoch 33/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 22.3341 - accuracy: 0.0000e+00 - val_loss: 29.1055 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00033: val_loss improved from 29.69091 to 29.10553, saving model to my_best_model.hdf5\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 3ms/step - loss: 22.3493 - accuracy: 0.0000e+00 - val_loss: 29.7231 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 29.10553\n",
      "Epoch 35/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 17.7104 - accuracy: 0.0000e+00 - val_loss: 28.8760 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00035: val_loss improved from 29.10553 to 28.87598, saving model to my_best_model.hdf5\n",
      "Epoch 36/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 32.1040 - accuracy: 0.0000e+00 - val_loss: 28.6607 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00036: val_loss improved from 28.87598 to 28.66074, saving model to my_best_model.hdf5\n",
      "Epoch 37/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 16.8013 - accuracy: 0.0000e+00 - val_loss: 28.1426 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00037: val_loss improved from 28.66074 to 28.14260, saving model to my_best_model.hdf5\n",
      "Epoch 38/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 26.0820 - accuracy: 0.0000e+00 - val_loss: 27.6576 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00038: val_loss improved from 28.14260 to 27.65762, saving model to my_best_model.hdf5\n",
      "Epoch 39/40\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 19.9387 - accuracy: 0.0000e+00 - val_loss: 27.5487 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00039: val_loss improved from 27.65762 to 27.54874, saving model to my_best_model.hdf5\n",
      "Epoch 40/40\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 18.7520 - accuracy: 0.0000e+00 - val_loss: 27.2787 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00040: val_loss improved from 27.54874 to 27.27873, saving model to my_best_model.hdf5\n",
      "  Model_Name       MAE      RMSE        R2\n",
      "0         RF  2.572336  3.940197  0.813546\n",
      "1        ANN  3.603087  5.222905  0.672389\n",
      "2        KNN  3.643026  5.774754  0.599501\n",
      "3         LR  3.703292  5.305355  0.661963\n"
     ]
    }
   ],
   "source": [
    "processing_obj = Models(filepath, columns_to_drop, target_column, test_size, model_list)\n",
    "processing_obj.data_processing()\n",
    "processing_obj.model_calls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
